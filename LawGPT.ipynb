{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the tutorial on [llamaindex](https://docs.llamaindex.ai/en/stable/examples/low_level/oss_ingestion_retrieval/) that using all free and open source packages that executable on colab without any oth registration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## installation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-readers-file pymupdf\n",
    "%pip install llama-index-vector-stores-postgres\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-llms-llama-cpp\n",
    "%pip install llama-index-llms-gemini\n",
    "%pip install llama-index\n",
    "%pip install 'google-generativeai==0.3.1'\n",
    "## install openai if you want to use semantic embedding\n",
    "# %pip install llama-index-embeddings-openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pgvector & postgresql\n",
    "Open-source vector similarity search for Postgres<br/>\n",
    "[Github](https://github.com/pgvector/pgvector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2-binary asyncpg \"sqlalchemy[asyncio]\" greenlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!git clone https://github.com/pgvector/pgvector.git\n",
    "\n",
    "!apt-get install -y postgresql-server-dev-14\n",
    "!apt-get install -y make gcc\n",
    "\n",
    "%cd pgvector\n",
    "!make && make install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API = \"<YOUR_GEMINI_API>\"\n",
    "embedding_model_type = \"Gemini\"\n",
    "chunk_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y postgresql postgresql-contrib\n",
    "# start PostgreSQL service\n",
    "import os\n",
    "os.system('service postgresql start')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Transformer (for document embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_model_factory(model_source=\"HuggingFace\", model_name=None, model_path=None, **kwarg):\n",
    "    if model_source==\"HuggingFace\":\n",
    "        from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "        if model_name is None:\n",
    "            model_name=\"BAAI/bge-small-en\"\n",
    "        embed_model = HuggingFaceEmbedding(model_name=model_name)\n",
    "    if model_source==\"Gemini\":\n",
    "        from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "        if model_name is None:\n",
    "            model_name=\"models/embedding-001\"\n",
    "        embed_model = GeminiEmbedding(api_key=GEMINI_API, model_name=\"models/embedding-001\")\n",
    "\n",
    "    return embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also try other model for embedding\n",
    "# sentence transformers\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# default model: HuggingFace, BAAI/bge-small-en\n",
    "embed_model = embed_model_factory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other adjustment\n",
    "[Order of evidence](https://arxiv.org/pdf/2305.13300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference [llamaindex gemini](https://docs.llamaindex.ai/en/stable/examples/llm/gemini/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.llms import ChatMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Test complete\n",
    "response = Gemini(api_key=GEMINI_API).complete(\"Hello world!\")\n",
    "print(response)\n",
    "\n",
    "# Test chat\n",
    "# Chat\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
    "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
    "    ChatMessage(\n",
    "        role=\"user\", content=\"Help me decide what to have for dinner.\"\n",
    "    ),\n",
    "]\n",
    "resp = Gemini().chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pg_hba_path = \"/etc/postgresql/14/main/pg_hba.conf\"\n",
    "\n",
    "with open(pg_hba_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "jerry_host_added = False\n",
    "### be careful when you adjust permission on your own computer\n",
    "# adjust the permission\n",
    "with open(pg_hba_path, \"w\") as file:\n",
    "    for line in lines:\n",
    "        if line.startswith(\"local   all             postgres\"):\n",
    "            file.write(\"local   all             postgres                                trust\\n\")\n",
    "        elif line.startswith(\"host    all             all             127.0.0.1/32\"):\n",
    "            file.write(\"host    all             all             127.0.0.1/32            trust\\n\")\n",
    "        elif line.startswith(\"host    all             all             ::1/128\"):\n",
    "            file.write(\"host    all             all             ::1/128                 trust\\n\")\n",
    "        else:\n",
    "            file.write(line)\n",
    "    if not jerry_host_added:\n",
    "        file.write(\"host    all             jerry            0.0.0.0/0               trust\\n\")\n",
    "        jerry_host_added = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('service postgresql restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psycopg2\n",
    "\n",
    "db_name = \"vector_db\"\n",
    "host = \"localhost\"\n",
    "password = \"password\"\n",
    "port = \"5432\"\n",
    "user = \"jerry\"\n",
    "# conn = psycopg2.connect(connection_string)\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "    c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"lawGPT\",\n",
    "    embed_dim=384,  # openai embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_parser = SentenceSplitter(chunk_size=chunk_size)\n",
    "\n",
    "# 定義資料夾路徑\n",
    "folder_path = 'path/to/your/json/folder'\n",
    "\n",
    "# 初始化節點列表\n",
    "nodes = []\n",
    "\n",
    "# 遍歷資料夾中的所有 JSON 文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # 處理每個 section\n",
    "        for section_data in data['sections']:\n",
    "            section = section_data['section']\n",
    "            content = section_data['content']\n",
    "            link = section_data['link']\n",
    "\n",
    "            # 使用 SentenceSplitter 將文本分塊\n",
    "            text_chunks = text_parser.split_text(content)\n",
    "\n",
    "            # 對每個文本塊進行 embedding 並創建節點\n",
    "            for idx, text_chunk in enumerate(text_chunks):\n",
    "                embedding = embed_model.get_text_embedding(text_chunk)\n",
    "                node = TextNode(\n",
    "                    text=text_chunk,\n",
    "                    embedding=embedding,\n",
    "                    metadata={\n",
    "                        \"section\": section,\n",
    "                        \"link\": link,\n",
    "                        \"filename\": filename  # 添加文件名作為 metadata\n",
    "                    }\n",
    "                )\n",
    "                nodes.append(node)\n",
    "\n",
    "# 節點列表 nodes 現在包含所有文件的所有節點\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
