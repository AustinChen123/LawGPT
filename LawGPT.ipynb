{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the tutorial on [llamaindex](https://docs.llamaindex.ai/en/stable/examples/low_level/oss_ingestion_retrieval/) that using all free and open source packages that executable on colab without any oth registration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## installation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-readers-file pymupdf\n",
    "%pip install llama-index-vector-stores-postgres\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-llms-llama-cpp\n",
    "%pip install llama-index-llms-gemini\n",
    "%pip install llama-index\n",
    "%pip install 'google-generativeai==0.3.1'\n",
    "## install openai if you want to use semantic embedding\n",
    "# %pip install llama-index-embeddings-openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pgvector & postgresql\n",
    "Open-source vector similarity search for Postgres<br/>\n",
    "[Github](https://github.com/pgvector/pgvector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2-binary asyncpg \"sqlalchemy[asyncio]\" greenlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!git clone https://github.com/pgvector/pgvector.git\n",
    "\n",
    "!apt-get install -y postgresql-server-dev-14\n",
    "!apt-get install -y make gcc\n",
    "\n",
    "%cd pgvector\n",
    "!make && make install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "in_colab = 'google.colab' in sys.modules\n",
    "if in_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # set folder_path to your drive\n",
    "    folder_path = '/content/drive/MyDrive/your_json_folder'\n",
    "else:\n",
    "    # if not Colab\n",
    "    folder_path = 'path/to/your/json/folder'\n",
    "\n",
    "# if in_colab:\n",
    "#     from google.colab import files\n",
    "#     uploaded = files.upload()\n",
    "#     # check if the folder exists\n",
    "#     import os\n",
    "#     os.makedirs(folder_path, exist_ok=True)\n",
    "#     # save file\n",
    "#     for filename, content in uploaded.items():\n",
    "#         with open(os.path.join(folder_path, filename), 'wb') as f:\n",
    "#             f.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API = \"<YOUR_GEMINI_API>\"  \n",
    "embedding_model_type = \"Gemini\"   # ÂèØ\"Gemini\" / \"HuggingFace\"\n",
    "chunk_size = 256                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y postgresql postgresql-contrib\n",
    "# start PostgreSQL service\n",
    "import os\n",
    "os.system('service postgresql start')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Transformer (for document embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_model_factory(model_source=\"HuggingFace\", model_name=None, model_path=None, **kwargs):\n",
    "    '''\n",
    "    specify the model source and model name to get certain model from various source\n",
    "    '''\n",
    "    if model_source == \"HuggingFace\":\n",
    "        from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "        if model_name is None:\n",
    "            model_name = \"BAAI/bge-small-en\"\n",
    "        embed_model = HuggingFaceEmbedding(model_name=model_name)\n",
    "    elif model_source == \"Gemini\":\n",
    "        from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "        if model_name is None:\n",
    "            model_name = \"models/embedding-001\"\n",
    "        embed_model = GeminiEmbedding(api_key=GEMINI_API, model_name=\"models/embedding-001\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported embedding model source: {model_source}\")\n",
    "    return embed_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model_factory(model_source=\"OpenAI\", model_name=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Specify the model source and model name to get a certain LLM from various sources.\n",
    "    \"\"\"\n",
    "    if model_source == \"OpenAI\":\n",
    "        from llama_index.llms import OpenAI\n",
    "        if model_name is None:\n",
    "            model_name = \"gpt-3.5-turbo\"\n",
    "        llm = OpenAI(\n",
    "            model=model_name,\n",
    "            api_key=OPENAI_API_KEY,  # OpenAI API key\n",
    "            **kwargs\n",
    "        )\n",
    "    elif model_source == \"Anthropic\":\n",
    "        from llama_index.llms import Anthropic\n",
    "        if model_name is None:\n",
    "            model_name = \"claude-v1\"\n",
    "        llm = Anthropic(\n",
    "            model=model_name,\n",
    "            api_key=ANTHROPIC_API_KEY,  # Anthropic API key\n",
    "            **kwargs\n",
    "        )\n",
    "    elif model_source == \"Gemini\":\n",
    "        # \n",
    "        # from llama_index.llms import Gemini\n",
    "        # llm = Gemini(\n",
    "        #     model=model_name,\n",
    "        #     api_key=GEMINI_API_KEY,  # Gemini API key\n",
    "        #     **kwargs\n",
    "        # )\n",
    "        pass  # Placeholder for Gemini LLM\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model source: {model_source}\")\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other adjustment\n",
    "[Order of evidence](https://arxiv.org/pdf/2305.13300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference [llamaindex gemini](https://docs.llamaindex.ai/en/stable/examples/llm/gemini/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.llms import ChatMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Test complete\n",
    "response = Gemini(api_key=GEMINI_API).complete(\"Hello world!\")\n",
    "print(response)\n",
    "\n",
    "# Test chat\n",
    "# Chat\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
    "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
    "    ChatMessage(\n",
    "        role=\"user\", content=\"Help me decide what to have for dinner.\"\n",
    "    ),\n",
    "]\n",
    "resp = Gemini().chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pg_hba_path = \"/etc/postgresql/14/main/pg_hba.conf\"\n",
    "\n",
    "with open(pg_hba_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "jerry_host_added = False\n",
    "### be careful when you adjust permission on your own computer\n",
    "# adjust the permission for database\n",
    "with open(pg_hba_path, \"w\") as file:\n",
    "    for line in lines:\n",
    "        if line.startswith(\"local   all             postgres\"):\n",
    "            file.write(\"local   all             postgres                                trust\\n\")\n",
    "        elif line.startswith(\"host    all             all             127.0.0.1/32\"):\n",
    "            file.write(\"host    all             all             127.0.0.1/32            trust\\n\")\n",
    "        elif line.startswith(\"host    all             all             ::1/128\"):\n",
    "            file.write(\"host    all             all             ::1/128                 trust\\n\")\n",
    "        else:\n",
    "            file.write(line)\n",
    "    if not jerry_host_added:\n",
    "        file.write(\"host    all             jerry            0.0.0.0/0               trust\\n\")\n",
    "        jerry_host_added = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('service postgresql restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psycopg2\n",
    "\n",
    "db_name = \"vector_db\"\n",
    "host = \"localhost\"\n",
    "password = \"password\"\n",
    "port = \"5432\"\n",
    "user = \"jerry\"\n",
    "# conn = psycopg2.connect(connection_string)\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "    c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"lawGPT\",\n",
    "    embed_dim=384,  # openai embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_parser = SentenceSplitter(chunk_size=chunk_size)\n",
    "embed_model = embed_model_factory(model_source=embedding_model_type)\n",
    "\n",
    "\n",
    "# init node\n",
    "nodes = []\n",
    "\n",
    "# retrive all json documents\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # handle by section\n",
    "        for section_data in data['sections']:\n",
    "            section = section_data['section']\n",
    "            content = section_data['content']\n",
    "            link = section_data['link']\n",
    "\n",
    "            # chunking\n",
    "            text_chunks = text_parser.split_text(content)\n",
    "\n",
    "            # embed each chunk and create node\n",
    "            for idx, text_chunk in enumerate(text_chunks):\n",
    "                embedding = embed_model.get_text_embedding(text_chunk)\n",
    "                node = TextNode(\n",
    "                    text=text_chunk,\n",
    "                    embedding=embedding,\n",
    "                    metadata={\n",
    "                        \"section\": section,\n",
    "                        \"link\": link,\n",
    "                        \"filename\": filename \n",
    "                    }\n",
    "                )\n",
    "                nodes.append(node)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also try other model for embedding\n",
    "# sentence transformers\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# default model: HuggingFace, BAAI/bge-small-en\n",
    "llm = llm_model_factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "# ÂàõÂª∫Êü•ËØ¢ÂºïÊìé\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Á§∫‰æãÊü•ËØ¢\n",
    "response = query_engine.query(\"What type of visa can I have if I just got my german university degree? How long is it?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
